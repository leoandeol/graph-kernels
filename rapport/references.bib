
@inproceedings{shervashidze_efficient_2009,
	title = {Efficient graphlet kernels for large graph comparison},
	url = {http://proceedings.mlr.press/v5/shervashidze09a.html},
	abstract = {State-of-the-art  graph kernels do not scale to large graphs with hundreds of nodes and thousands of edges. In this article we propose to compare graphs by counting  {\textbackslash}it graphlets, {\textbackslash}ie subgraphs wi...},
	language = {en},
	urldate = {2019-01-03},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	author = {Shervashidze, Nino and Vishwanathan, S. V. N. and Petri, Tobias and Mehlhorn, Kurt and Borgwardt, Karsten},
	month = apr,
	year = {2009},
	pages = {488--495},
	file = {Full Text PDF:/home/leo/Zotero/storage/9XJVNTGC/Shervashidze et al. - 2009 - Efficient graphlet kernels for large graph compari.pdf:application/pdf;Snapshot:/home/leo/Zotero/storage/W788B6VK/shervashidze09a.html:text/html}
}

@article{vishwanathan_graph_2010,
	title = {Graph {Kernels}},
	volume = {11},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v11/vishwanathan10a.html},
	number = {Apr},
	urldate = {2019-01-03},
	journal = {Journal of Machine Learning Research},
	author = {Vishwanathan, S. V. N. and Schraudolph, Nicol N. and Kondor, Risi and Borgwardt, Karsten M.},
	year = {2010},
	pages = {1201--1242},
	file = {Full Text PDF:/home/leo/Zotero/storage/76LCGZY9/Vishwanathan et al. - 2010 - Graph Kernels.pdf:application/pdf;Snapshot:/home/leo/Zotero/storage/JZNKUHT9/vishwanathan10a.html:text/html}
}

@article{burges_tutorial_1998,
	title = {A {Tutorial} on {Support} {Vector} {Machines} for {Pattern} {Recognition}},
	volume = {2},
	issn = {1384-5810},
	url = {https://doi.org/10.1023/A:1009715923555},
	doi = {10.1023/A:1009715923555},
	abstract = {The tutorial starts with an overview of the concepts of VC dimension
	and structural risk minimization. We then describe linear Support Vector
	Machines (SVMs) for separable and non-separable data, working through a
	non-trivial example in detail. We describe a mechanical analogy, and
	discuss when SVM solutions are unique and when they are global. We describe
	how support vector training can be practically implemented, and discuss in
	detail the kernel mapping technique which is used to construct SVM
	solutions which are nonlinear in the data. We show how Support Vector
	machines can have very large (even infinite) VC dimension by computing the
	VC dimension for homogeneous polynomial and Gaussian radial basis function
	kernels. While very high VC dimension would normally bode ill for
	generalization performance, and while at present there exists no theory
	which shows that good generalization performance is guaranteed for SVMs,
	there are several arguments which support the observed high accuracy of
	SVMs, which we review. Results of some experiments which were inspired by
	these arguments are also presented. We give numerous examples and proofs of
	most of the key theorems. There is new material, and I hope that the reader
	will find that even old material is cast in a fresh light.},
	number = {2},
	urldate = {2019-01-29},
	journal = {Data Min. Knowl. Discov.},
	author = {Burges, Christopher J. C.},
	month = jun,
	year = {1998},
	keywords = {pattern recognition, statistical learning theory, support vector machines, VC dimension},
	pages = {121--167}
}

@book{schlkopf_learning_2001,
	address = {Cambridge, Mass},
	edition = {1st edition},
	title = {Learning with {Kernels}: {Support} {Vector} {Machines}, {Regularization}, {Optimization}, and {Beyond}},
	isbn = {978-0-262-19475-4},
	shorttitle = {Learning with {Kernels}},
	abstract = {A comprehensive introduction to Support Vector Machines and related kernel methods.In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs―-kernels―for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics.Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.},
	language = {English},
	publisher = {The MIT Press},
	author = {Schlkopf, Bernhard and Smola, Alexander J.},
	month = dec,
	year = {2001}
}

@book{vapnik_statistical_1998,
	address = {New York},
	title = {Statistical {Learning} {Theory}},
	isbn = {978-0-471-03003-4},
	abstract = {A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real–life problems, and much more.},
	language = {Anglais},
	publisher = {Wiley-Blackwell},
	author = {Vapnik, Vladimir N.},
	month = oct,
	year = {1998}
}

@article{rossi_estimation_2019,
	title = {Estimation of {Graphlet} {Counts} in {Massive} {Networks}},
	volume = {30},
	issn = {2162-237X},
	doi = {10.1109/TNNLS.2018.2826529},
	abstract = {Graphlets are induced subgraphs of a large network and are important for understanding and modeling complex networks. Despite their practical importance, graphlets have been severely limited to applications and domains with relatively small graphs. Most previous work has focused on exact algorithms; however, it is often too expensive to compute graphlets exactly in massive networks with billions of edges, and finding an approximate count is usually sufficient for many applications. In this paper, we propose an unbiased graphlet estimation framework that is: (a) fast with large speedups compared to the state of the art; (b) parallel with nearly linear speedups; (c) accurate with less than 1\% relative error; (d) scalable and space efficient for massive networks with billions of edges; and (e) effective for a variety of real-world settings as well as estimating global and local graphlet statistics (e.g., counts). On 300 networks from 20 domains, we obtain {\textless};1\% relative error for all graphlets. This is vastly more accurate than the existing methods while using less data. Moreover, it takes a few seconds on billion edge graphs (as opposed to days/weeks). These are by far the largest graphlet computations to date.},
	number = {1},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Rossi, R. A. and Zhou, R. and Ahmed, N. K.},
	month = jan,
	year = {2019},
	keywords = {Approximation algorithms, Art, complex network modeling, complex networks, edge graphs, Estimation, estimation methods, global graphlet statistics, graph theory, graphlet count estimation, graphlet statistics, Graphlets, higher-order network analysis, Indexes, induced subgraphs, largest graphlet computations, Learning systems, local graphlet count estimation, local graphlet statistics, machine learning, Machine learning algorithms, massive networks, network motifs, network theory (graphs), parallel algorithms, Parallel algorithms, unbiased graphlet estimation},
	pages = {44--57},
	file = {IEEE Xplore Abstract Record:/home/leo/Zotero/storage/9AKCJ47K/8361082.html:text/html;IEEE Xplore Full Text PDF:/home/leo/Zotero/storage/JFXZ6TFF/Rossi et al. - 2019 - Estimation of Graphlet Counts in Massive Networks.pdf:application/pdf}
}

@book{nesterov_lectures_2018,
	edition = {2},
	series = {Springer {Optimization} and {Its} {Applications}},
	title = {Lectures on {Convex} {Optimization}},
	isbn = {978-3-319-91577-7},
	url = {https://www.springer.com/us/book/9783319915777},
	abstract = {This book provides a comprehensive, modern introduction to convex optimization, a field that is becoming increasingly important in applied mathematics, economics and finance, engineering, and computer science, notably in data science and machine learning. Written by a leading expert in the field, this book includes recent advances in the algorithmic theory of convex optimization, naturally complementing the existing literature. It contains a unified and rigorous presentation of the acceleration techniques for minimization schemes of first- and second-order. It provides readers with a full treatment of the smoothing technique, which has tremendously extended the abilities of gradient-type methods. Several powerful approaches in structural optimization, including optimization in relative scale and polynomial-time interior-point methods, are also discussed in detail. Researchers in theoretical optimization as well as professionals working on optimization problems will find this book very useful. It presents many successful examples of how to develop very fast specialized minimization algorithms. Based on the author’s lectures, it can naturally serve as the basis for introductory and advanced courses in convex optimization for students in engineering, economics, computer science and mathematics.},
	language = {en},
	urldate = {2019-03-17},
	publisher = {Springer International Publishing},
	author = {Nesterov, Yurii},
	year = {2018},
	file = {Snapshot:/home/leo/Zotero/storage/XRN4YD7A/9783319915777.html:text/html}
}

@article{borgwardt_protein_2005,
	title = {Protein function prediction via graph kernels},
	volume = {21 Suppl 1},
	issn = {1367-4803},
	doi = {10.1093/bioinformatics/bti1007},
	abstract = {MOTIVATION: Computational approaches to protein function prediction infer protein function by finding proteins with similar sequence, structure, surface clefts, chemical properties, amino acid motifs, interaction partners or phylogenetic profiles. We present a new approach that combines sequential, structural and chemical information into one graph model of proteins. We predict functional class membership of enzymes and non-enzymes using graph kernels and support vector machine classification on these protein graphs.
	RESULTS: Our graph model, derivable from protein sequence and structure only, is competitive with vector models that require additional protein information, such as the size of surface pockets. If we include this extra information into our graph model, our classifier yields significantly higher accuracy levels than the vector models. Hyperkernels allow us to select and to optimally combine the most relevant node attributes in our protein graphs. We have laid the foundation for a protein function prediction system that integrates protein information from various sources efficiently and effectively.
	AVAILABILITY: More information available via www.dbs.ifi.lmu.de/Mitarbeiter/borgwardt.html.},
	language = {eng},
	journal = {Bioinformatics (Oxford, England)},
	author = {Borgwardt, Karsten M. and Ong, Cheng Soon and Schönauer, Stefan and Vishwanathan, S. V. N. and Smola, Alex J. and Kriegel, Hans-Peter},
	month = jun,
	year = {2005},
	pmid = {15961493},
	keywords = {Algorithms, Computational Biology, Databases, Protein, Enzymes, Models, Statistical, Protein Conformation, Protein Structure, Secondary, Sequence Analysis, Protein, Software},
	pages = {i47--56},
	file = {Full Text:/home/leo/Zotero/storage/23H5ILHG/Borgwardt et al. - 2005 - Protein function prediction via graph kernels.pdf:application/pdf}
}

@article{cortes_support-vector_1995,
	title = {Support-vector networks},
	volume = {20},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00994018},
	doi = {10.1007/BF00994018},
	abstract = {Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.},
	language = {en},
	number = {3},
	urldate = {2019-03-17},
	journal = {Machine Learning},
	author = {Cortes, Corinna and Vapnik, Vladimir},
	month = sep,
	year = {1995},
	keywords = {neural networks, pattern recognition, efficient learning algorithms, polynomial classifiers, radial basis function classifiers},
	pages = {273--297},
	file = {Springer Full Text PDF:/home/leo/Zotero/storage/FPFKUMYJ/Cortes and Vapnik - 1995 - Support-vector networks.pdf:application/pdf}
}

@phdthesis{shervashidze2012scalable,
	title={Scalable graph kernels},
	author={Shervashidze, Nino},
	year={2012},
	school={Universit{\"a}t T{\"u}bingen}
}


@article{barrett_inverse_2017,
	title = {The inverse eigenvalue problem of a graph: {Multiplicities} and minors},
	shorttitle = {The inverse eigenvalue problem of a graph},
	url = {http://arxiv.org/abs/1708.00064},
	abstract = {The inverse eigenvalue problem of a given graph G is to determine all possible spectra of real symmetric matrices whose oﬀ-diagonal entries are governed by the adjacencies in G. Barrett et al. introduced the Strong Spectral Property (SSP) and the Strong Multiplicity Property (SMP) in [8]. In that paper it was shown that if a graph has a matrix with the SSP (or the SMP) then a supergraph has a matrix with the same spectrum (or ordered multiplicity list) augmented with simple eigenvalues if necessary, that is, subgraph monotonicity. In this paper we extend this to a form of minor monotonicity, with restrictions on where the new eigenvalues appear. These ideas are applied to solve the inverse eigenvalue problem for all graphs of order ﬁve, and to characterize forbidden minors of graphs having at most one multiple eigenvalue.},
	language = {en},
	urldate = {2019-03-28},
	journal = {arXiv:1708.00064 [math]},
	author = {Barrett, Wayne and Butler, Steve and Fallat, Shaun M. and Hall, H. Tracy and Hogben, Leslie and Lin, Jephian C.-H. and Shader, Bryan L. and Young, Michael},
	month = jul,
	year = {2017},
	note = {arXiv: 1708.00064},
	keywords = {05C83, 05C50, 15A18, 15A29, 26B10, 58C15, Mathematics - Combinatorics},
	file = {Barrett et al. - 2017 - The inverse eigenvalue problem of a graph Multipl.pdf:/home/leo/Zotero/storage/2989F8QY/Barrett et al. - 2017 - The inverse eigenvalue problem of a graph Multipl.pdf:application/pdf}
}


@article{dobson_distinguishing_2003,
	title = {Distinguishing enzyme structures from non-enzymes without alignments},
	volume = {330},
	issn = {0022-2836},
	abstract = {The ability to predict protein function from structure is becoming increasingly important as the number of structures resolved is growing more rapidly than our capacity to study function. Current methods for predicting protein function are mostly reliant on identifying a similar protein of known function. For proteins that are highly dissimilar or are only similar to proteins also lacking functional annotations, these methods fail. Here, we show that protein function can be predicted as enzymatic or not without resorting to alignments. We describe 1178 high-resolution proteins in a structurally non-redundant subset of the Protein Data Bank using simple features such as secondary-structure content, amino acid propensities, surface properties and ligands. The subset is split into two functional groupings, enzymes and non-enzymes. We use the support vector machine-learning algorithm to develop models that are capable of assigning the protein class. Validation of the method shows that the function can be predicted to an accuracy of 77\% using 52 features to describe each protein. An adaptive search of possible subsets of features produces a simplified model based on 36 features that predicts at an accuracy of 80\%. We compare the method to sequence-based methods that also avoid calculating alignments and predict a recently released set of unrelated proteins. The most useful features for distinguishing enzymes from non-enzymes are secondary-structure content, amino acid frequencies, number of disulphide bonds and size of the largest cleft. This method is applicable to any structure as it does not require the identification of sequence or structural similarity to a protein of known function.},
	language = {eng},
	number = {4},
	journal = {Journal of Molecular Biology},
	author = {Dobson, Paul D. and Doig, Andrew J.},
	month = jul,
	year = {2003},
	pmid = {12850146},
	keywords = {Algorithms, Computational Biology, Enzymes, Protein Conformation, Protein Structure, Secondary, Software, Aspartic Acid, Databases as Topic, Genome, Ligands, Phenylalanine, Proteome},
	pages = {771--783}
}

@misc{graph_wiki,
	title = {Graph theory - {Wikipedia}},
	url = {https://en.wikipedia.org/wiki/Graph_theory},
	urldate = {2019-05-08},
	author = {Wikipedia},
	year = {2019},
	file = {Graph theory - Wikipedia:/home/leo/Zotero/storage/ZXZ2VJRV/Graph_theory.html:text/html}
}

@inproceedings{kashima_graphkers_2003,
	author = {Kashima, Hisashi and Tsuda, Koji and Inokuchi, Akihiro},
	title = {Marginalized Kernels Between Labeled Graphs},
	booktitle = {Proceedings of the Twentieth International Conference on International Conference on Machine Learning},
	series = {ICML'03},
	year = {2003},
	isbn = {1-57735-189-4},
	location = {Washington, DC, USA},
	pages = {321--328},
	numpages = {8},
	url = {http://dl.acm.org/citation.cfm?id=3041838.3041879},
	acmid = {3041879},
	publisher = {AAAI Press},
} 

@techreport{haussler99convolution,
	added-at = {2011-02-11T11:29:10.000+0100},
	address = {Santa Cruz, CA, USA},
	author = {Haussler, David},
	biburl = {https://www.bibsonomy.org/bibtex/266f4c8b9c3769f1718cade8f022096a4/utahell},
	description = {Convolution Kernels on Discrete Structures - Haussler (ResearchIndex)},
	institution = {University of California at Santa Cruz},
	interhash = {d730a29e3ae9a3ef0de3ff6199a4fd98},
	intrahash = {66f4c8b9c3769f1718cade8f022096a4},
	keywords = {kernel structured-data},
	number = {UCS-CRL-99-10},
	timestamp = {2011-10-28T16:52:50.000+0200},
	title = {Convolution Kernels on Discrete Structures},
	type = {Technical Report},
	url = {http://citeseer.ist.psu.edu/haussler99convolution.html},
	year = 1999
}

@incollection{van1993approximation,
	title={Approximation with Kronecker products},
	author={Van Loan, Charles F and Pitsianis, Nikos},
	booktitle={Linear algebra for large scale and real-time applications},
	pages={293--314},
	year={1993},
	publisher={Springer}
}


@article{lathauwer2004,
	author = {De Lathauwer, L. and De Moor, B. and Vandewalle, J.},
	title = {Computation of the Canonical Decomposition by Means of a Simultaneous Generalized Schur Decomposition},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	volume = {26},
	number = {2},
	pages = {295-327},
	year = {2004},
	doi = {10.1137/S089547980139786X},
	
	URL = { 
	https://doi.org/10.1137/S089547980139786X
	
	},
	eprint = { 
	https://doi.org/10.1137/S089547980139786X
	
	}
	
}
@book{bondy1976graph,
	title={Graph theory with applications},
	author={Bondy, John Adrian and Murty, Uppaluri Siva Ramachandra and others},
	volume={290},
	year={1976},
	publisher={Citeseer}
}

@article{weichsel1962kronecker,
	title={The Kronecker product of graphs},
	author={Weichsel, Paul M},
	journal={Proceedings of the American mathematical society},
	volume={13},
	number={1},
	pages={47--52},
	year={1962},
	publisher={JSTOR}
}

@article{freund1999large,
	title={Large margin classification using the perceptron algorithm},
	author={Freund, Yoav and Schapire, Robert E},
	journal={Machine learning},
	volume={37},
	number={3},
	pages={277--296},
	year={1999},
	publisher={Springer}
}


