
@inproceedings{shervashidze_efficient_2009,
	title = {Efficient graphlet kernels for large graph comparison},
	url = {http://proceedings.mlr.press/v5/shervashidze09a.html},
	abstract = {State-of-the-art  graph kernels do not scale to large graphs with hundreds of nodes and thousands of edges. In this article we propose to compare graphs by counting  {\textbackslash}it graphlets, {\textbackslash}ie subgraphs wi...},
	language = {en},
	urldate = {2019-01-03},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	author = {Shervashidze, Nino and Vishwanathan, S. V. N. and Petri, Tobias and Mehlhorn, Kurt and Borgwardt, Karsten},
	month = apr,
	year = {2009},
	pages = {488--495},
	file = {Full Text PDF:/home/leo/Zotero/storage/9XJVNTGC/Shervashidze et al. - 2009 - Efficient graphlet kernels for large graph compari.pdf:application/pdf;Snapshot:/home/leo/Zotero/storage/W788B6VK/shervashidze09a.html:text/html}
}

@article{vishwanathan_graph_2010,
	title = {Graph {Kernels}},
	volume = {11},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v11/vishwanathan10a.html},
	number = {Apr},
	urldate = {2019-01-03},
	journal = {Journal of Machine Learning Research},
	author = {Vishwanathan, S. V. N. and Schraudolph, Nicol N. and Kondor, Risi and Borgwardt, Karsten M.},
	year = {2010},
	pages = {1201--1242},
	file = {Full Text PDF:/home/leo/Zotero/storage/76LCGZY9/Vishwanathan et al. - 2010 - Graph Kernels.pdf:application/pdf;Snapshot:/home/leo/Zotero/storage/JZNKUHT9/vishwanathan10a.html:text/html}
}

@article{burges_tutorial_1998,
	title = {A {Tutorial} on {Support} {Vector} {Machines} for {Pattern} {Recognition}},
	volume = {2},
	issn = {1384-5810},
	url = {https://doi.org/10.1023/A:1009715923555},
	doi = {10.1023/A:1009715923555},
	abstract = {The tutorial starts with an overview of the concepts of VC dimension
	and structural risk minimization. We then describe linear Support Vector
	Machines (SVMs) for separable and non-separable data, working through a
	non-trivial example in detail. We describe a mechanical analogy, and
	discuss when SVM solutions are unique and when they are global. We describe
	how support vector training can be practically implemented, and discuss in
	detail the kernel mapping technique which is used to construct SVM
	solutions which are nonlinear in the data. We show how Support Vector
	machines can have very large (even infinite) VC dimension by computing the
	VC dimension for homogeneous polynomial and Gaussian radial basis function
	kernels. While very high VC dimension would normally bode ill for
	generalization performance, and while at present there exists no theory
	which shows that good generalization performance is guaranteed for SVMs,
	there are several arguments which support the observed high accuracy of
	SVMs, which we review. Results of some experiments which were inspired by
	these arguments are also presented. We give numerous examples and proofs of
	most of the key theorems. There is new material, and I hope that the reader
	will find that even old material is cast in a fresh light.},
	number = {2},
	urldate = {2019-01-29},
	journal = {Data Min. Knowl. Discov.},
	author = {Burges, Christopher J. C.},
	month = jun,
	year = {1998},
	keywords = {pattern recognition, statistical learning theory, support vector machines, VC dimension},
	pages = {121--167}
}

@book{schlkopf_learning_2001,
	address = {Cambridge, Mass},
	edition = {1st edition},
	title = {Learning with {Kernels}: {Support} {Vector} {Machines}, {Regularization}, {Optimization}, and {Beyond}},
	isbn = {978-0-262-19475-4},
	shorttitle = {Learning with {Kernels}},
	abstract = {A comprehensive introduction to Support Vector Machines and related kernel methods.In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs―-kernels―for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics.Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.},
	language = {English},
	publisher = {The MIT Press},
	author = {Schlkopf, Bernhard and Smola, Alexander J.},
	month = dec,
	year = {2001}
}

@book{vapnik_statistical_1998,
	address = {New York},
	title = {Statistical {Learning} {Theory}},
	isbn = {978-0-471-03003-4},
	abstract = {A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real–life problems, and much more.},
	language = {Anglais},
	publisher = {Wiley-Blackwell},
	author = {Vapnik, Vladimir N.},
	month = oct,
	year = {1998}
}

@article{rossi_estimation_2019,
	title = {Estimation of {Graphlet} {Counts} in {Massive} {Networks}},
	volume = {30},
	issn = {2162-237X},
	doi = {10.1109/TNNLS.2018.2826529},
	abstract = {Graphlets are induced subgraphs of a large network and are important for understanding and modeling complex networks. Despite their practical importance, graphlets have been severely limited to applications and domains with relatively small graphs. Most previous work has focused on exact algorithms; however, it is often too expensive to compute graphlets exactly in massive networks with billions of edges, and finding an approximate count is usually sufficient for many applications. In this paper, we propose an unbiased graphlet estimation framework that is: (a) fast with large speedups compared to the state of the art; (b) parallel with nearly linear speedups; (c) accurate with less than 1\% relative error; (d) scalable and space efficient for massive networks with billions of edges; and (e) effective for a variety of real-world settings as well as estimating global and local graphlet statistics (e.g., counts). On 300 networks from 20 domains, we obtain {\textless};1\% relative error for all graphlets. This is vastly more accurate than the existing methods while using less data. Moreover, it takes a few seconds on billion edge graphs (as opposed to days/weeks). These are by far the largest graphlet computations to date.},
	number = {1},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Rossi, R. A. and Zhou, R. and Ahmed, N. K.},
	month = jan,
	year = {2019},
	keywords = {Approximation algorithms, Art, complex network modeling, complex networks, edge graphs, Estimation, estimation methods, global graphlet statistics, graph theory, graphlet count estimation, graphlet statistics, Graphlets, higher-order network analysis, Indexes, induced subgraphs, largest graphlet computations, Learning systems, local graphlet count estimation, local graphlet statistics, machine learning, Machine learning algorithms, massive networks, network motifs, network theory (graphs), parallel algorithms, Parallel algorithms, unbiased graphlet estimation},
	pages = {44--57},
	file = {IEEE Xplore Abstract Record:/home/leo/Zotero/storage/9AKCJ47K/8361082.html:text/html;IEEE Xplore Full Text PDF:/home/leo/Zotero/storage/JFXZ6TFF/Rossi et al. - 2019 - Estimation of Graphlet Counts in Massive Networks.pdf:application/pdf}
}

@book{nesterov_lectures_2018,
	edition = {2},
	series = {Springer {Optimization} and {Its} {Applications}},
	title = {Lectures on {Convex} {Optimization}},
	isbn = {978-3-319-91577-7},
	url = {https://www.springer.com/us/book/9783319915777},
	abstract = {This book provides a comprehensive, modern introduction to convex optimization, a field that is becoming increasingly important in applied mathematics, economics and finance, engineering, and computer science, notably in data science and machine learning. Written by a leading expert in the field, this book includes recent advances in the algorithmic theory of convex optimization, naturally complementing the existing literature. It contains a unified and rigorous presentation of the acceleration techniques for minimization schemes of first- and second-order. It provides readers with a full treatment of the smoothing technique, which has tremendously extended the abilities of gradient-type methods. Several powerful approaches in structural optimization, including optimization in relative scale and polynomial-time interior-point methods, are also discussed in detail. Researchers in theoretical optimization as well as professionals working on optimization problems will find this book very useful. It presents many successful examples of how to develop very fast specialized minimization algorithms. Based on the author’s lectures, it can naturally serve as the basis for introductory and advanced courses in convex optimization for students in engineering, economics, computer science and mathematics.},
	language = {en},
	urldate = {2019-03-17},
	publisher = {Springer International Publishing},
	author = {Nesterov, Yurii},
	year = {2018},
	file = {Snapshot:/home/leo/Zotero/storage/XRN4YD7A/9783319915777.html:text/html}
}

@article{borgwardt_protein_2005,
	title = {Protein function prediction via graph kernels},
	volume = {21 Suppl 1},
	issn = {1367-4803},
	doi = {10.1093/bioinformatics/bti1007},
	abstract = {MOTIVATION: Computational approaches to protein function prediction infer protein function by finding proteins with similar sequence, structure, surface clefts, chemical properties, amino acid motifs, interaction partners or phylogenetic profiles. We present a new approach that combines sequential, structural and chemical information into one graph model of proteins. We predict functional class membership of enzymes and non-enzymes using graph kernels and support vector machine classification on these protein graphs.
	RESULTS: Our graph model, derivable from protein sequence and structure only, is competitive with vector models that require additional protein information, such as the size of surface pockets. If we include this extra information into our graph model, our classifier yields significantly higher accuracy levels than the vector models. Hyperkernels allow us to select and to optimally combine the most relevant node attributes in our protein graphs. We have laid the foundation for a protein function prediction system that integrates protein information from various sources efficiently and effectively.
	AVAILABILITY: More information available via www.dbs.ifi.lmu.de/Mitarbeiter/borgwardt.html.},
	language = {eng},
	journal = {Bioinformatics (Oxford, England)},
	author = {Borgwardt, Karsten M. and Ong, Cheng Soon and Schönauer, Stefan and Vishwanathan, S. V. N. and Smola, Alex J. and Kriegel, Hans-Peter},
	month = jun,
	year = {2005},
	pmid = {15961493},
	keywords = {Algorithms, Computational Biology, Databases, Protein, Enzymes, Models, Statistical, Protein Conformation, Protein Structure, Secondary, Sequence Analysis, Protein, Software},
	pages = {i47--56},
	file = {Full Text:/home/leo/Zotero/storage/23H5ILHG/Borgwardt et al. - 2005 - Protein function prediction via graph kernels.pdf:application/pdf}
}

@article{cortes_support-vector_1995,
	title = {Support-vector networks},
	volume = {20},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00994018},
	doi = {10.1007/BF00994018},
	abstract = {Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.},
	language = {en},
	number = {3},
	urldate = {2019-03-17},
	journal = {Machine Learning},
	author = {Cortes, Corinna and Vapnik, Vladimir},
	month = sep,
	year = {1995},
	keywords = {neural networks, pattern recognition, efficient learning algorithms, polynomial classifiers, radial basis function classifiers},
	pages = {273--297},
	file = {Springer Full Text PDF:/home/leo/Zotero/storage/FPFKUMYJ/Cortes and Vapnik - 1995 - Support-vector networks.pdf:application/pdf}
}

@phdthesis{shervashidze_scalable_2012,
	type = {Dissertation},
	title = {Scalable graph kernels},
	copyright = {http://tobias-lib.uni-tuebingen.de/doku/lic\_mit\_pod.php?la=de},
	url = {https://publikationen.uni-tuebingen.de/xmlui/handle/10900/49731},
	abstract = {Graph-structured data are becoming more and more abundant in many fields of science and engineering, such as social network analysis, molecular biology, chemistry, computer vision, or program verification. To exploit these data, one needs data analysis and machine learning methods that are able to efficiently handle large-scale graph data sets. Successfully applying machine learning and data analysis methods to graphs requires the ability to efficiently compare and represent graphs. Standard solutions to these problems are either NP-hard, not expressive enough, or difficult to adapt to a problem at hand.
	
	Graph kernels have attracted considerable interest in the machine learning community in the last decade as a promising solution to the above-mentioned issues. Despite significant progress in the design and improvement of graph kernels in the past few years, existing graph kernels do not measure up to the current needs of machine learning on large, labeled graphs: Even the most efficient existing kernels need O(n{\textasciicircum}3) runtime to compare a pair of graphs with n nodes, or cannot take into account node and edge labels. Our primary goal in this thesis is the design of efficient and expressive kernels for machine learning on graphs.
	
	We first focus on the design of generic graph kernels that can be applied to graphs with or without labels. Our main contributions to this end are the following: 
	
	First, we speed up the exact computation of graphlet kernels from O(n{\textasciicircum}k) to O(nd{\textasciicircum}\{k-1\}) for a pair of graphs, where n is the size of the graphs, k is the size of considered graphlets, and d is the maximum degree in the given graphs.
	
	Second, we define a new kernel on graphs, the Weisfeiler-Lehman subtree kernel, which is the first graph kernel scaling linearly in the number of edges in the given graph set. In our experiments on benchmark graph data sets from chemoinformatics and bioinformatics, the Weisfeiler-Lehman subtree kernel gracefully scales up to large graphs, outperforms all existing graph kernels in speed, and yields highly competitive performance in graph classification.
	
	Third, we generalize the Weisfeiler-Lehman subtree kernel to a family of kernels that includes many known graph kernels as special cases. This generalization enables existing graph kernels to take into account more information about the graph topology, and thereby become more expressive.
	
	In the last part of this thesis, we present two examples of applications: Based on our previous contributions, we propose specialized node kernels for pixel classification in remote sensing images, and graph kernels for chemical shift prediction in structural bioinformatics. Our kernels make it possible for the first time to take advantage of the rich graph structure in these applications.
	
	The Weisfeiler-Lehman kernels we propose here now allow graph kernels to scale to large, labeled graphs. They open the door to manifold applications of graph kernels in numerous domains which deal with graphs whose size and attributes could not be handled by graph kernels before.},
	language = {en},
	urldate = {2019-03-25},
	school = {Universität Tübingen},
	author = {Shervashidze, Nino},
	year = {2012},
	file = {Full Text PDF:/home/leo/Zotero/storage/UUZHRWXS/Shervashidze - 2012 - Scalable graph kernels.pdf:application/pdf;Snapshot:/home/leo/Zotero/storage/UCQGF9DU/49731.html:text/html}
}

@article{barrett_inverse_2017,
	title = {The inverse eigenvalue problem of a graph: {Multiplicities} and minors},
	shorttitle = {The inverse eigenvalue problem of a graph},
	url = {http://arxiv.org/abs/1708.00064},
	abstract = {The inverse eigenvalue problem of a given graph G is to determine all possible spectra of real symmetric matrices whose oﬀ-diagonal entries are governed by the adjacencies in G. Barrett et al. introduced the Strong Spectral Property (SSP) and the Strong Multiplicity Property (SMP) in [8]. In that paper it was shown that if a graph has a matrix with the SSP (or the SMP) then a supergraph has a matrix with the same spectrum (or ordered multiplicity list) augmented with simple eigenvalues if necessary, that is, subgraph monotonicity. In this paper we extend this to a form of minor monotonicity, with restrictions on where the new eigenvalues appear. These ideas are applied to solve the inverse eigenvalue problem for all graphs of order ﬁve, and to characterize forbidden minors of graphs having at most one multiple eigenvalue.},
	language = {en},
	urldate = {2019-03-28},
	journal = {arXiv:1708.00064 [math]},
	author = {Barrett, Wayne and Butler, Steve and Fallat, Shaun M. and Hall, H. Tracy and Hogben, Leslie and Lin, Jephian C.-H. and Shader, Bryan L. and Young, Michael},
	month = jul,
	year = {2017},
	note = {arXiv: 1708.00064},
	keywords = {05C83, 05C50, 15A18, 15A29, 26B10, 58C15, Mathematics - Combinatorics},
	file = {Barrett et al. - 2017 - The inverse eigenvalue problem of a graph Multipl.pdf:/home/leo/Zotero/storage/2989F8QY/Barrett et al. - 2017 - The inverse eigenvalue problem of a graph Multipl.pdf:application/pdf}
}


@misc{noauthor_kronecker_2019,
	title = {Kronecker product},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Kronecker_product&oldid=887195556},
	abstract = {In mathematics, the Kronecker product, denoted by ⊗, is an operation on two matrices of arbitrary size resulting in a block matrix. It is a generalization of the outer product (which is denoted by the same symbol) from vectors to matrices, and gives the matrix of the tensor product with respect to a standard choice of basis. The Kronecker product should not be confused with the usual matrix multiplication, which is an entirely different operation.
	The Kronecker product is named after Leopold Kronecker, even though there is little evidence that he was the first to define and use it. Indeed, in the past the Kronecker product was sometimes called the Zehfuss matrix, after Johann Georg Zehfuss who in 1858 described the matrix operation we now know as the Kronecker product.},
	language = {en},
	urldate = {2019-05-08},
	journal = {Wikipedia},
	author = {Wikipedia},
	month = mar,
	year = {2019},
	note = {Page Version ID: 887195556},
	file = {Snapshot:/home/leo/Zotero/storage/FLG3A3UU/index.html:text/html}
}

@article{dobson_distinguishing_2003,
	title = {Distinguishing enzyme structures from non-enzymes without alignments},
	volume = {330},
	issn = {0022-2836},
	abstract = {The ability to predict protein function from structure is becoming increasingly important as the number of structures resolved is growing more rapidly than our capacity to study function. Current methods for predicting protein function are mostly reliant on identifying a similar protein of known function. For proteins that are highly dissimilar or are only similar to proteins also lacking functional annotations, these methods fail. Here, we show that protein function can be predicted as enzymatic or not without resorting to alignments. We describe 1178 high-resolution proteins in a structurally non-redundant subset of the Protein Data Bank using simple features such as secondary-structure content, amino acid propensities, surface properties and ligands. The subset is split into two functional groupings, enzymes and non-enzymes. We use the support vector machine-learning algorithm to develop models that are capable of assigning the protein class. Validation of the method shows that the function can be predicted to an accuracy of 77\% using 52 features to describe each protein. An adaptive search of possible subsets of features produces a simplified model based on 36 features that predicts at an accuracy of 80\%. We compare the method to sequence-based methods that also avoid calculating alignments and predict a recently released set of unrelated proteins. The most useful features for distinguishing enzymes from non-enzymes are secondary-structure content, amino acid frequencies, number of disulphide bonds and size of the largest cleft. This method is applicable to any structure as it does not require the identification of sequence or structural similarity to a protein of known function.},
	language = {eng},
	number = {4},
	journal = {Journal of Molecular Biology},
	author = {Dobson, Paul D. and Doig, Andrew J.},
	month = jul,
	year = {2003},
	pmid = {12850146},
	keywords = {Algorithms, Computational Biology, Enzymes, Protein Conformation, Protein Structure, Secondary, Software, Aspartic Acid, Databases as Topic, Genome, Ligands, Phenylalanine, Proteome},
	pages = {771--783}
}

@misc{graph_wiki,
	title = {Graph theory - {Wikipedia}},
	url = {https://en.wikipedia.org/wiki/Graph_theory},
	urldate = {2019-05-08},
	author = {Wikipedia},
	year = {2019},
	file = {Graph theory - Wikipedia:/home/leo/Zotero/storage/ZXZ2VJRV/Graph_theory.html:text/html}
}

@inproceedings{kashima_graphkers_2003,
	author = {Kashima, Hisashi and Tsuda, Koji and Inokuchi, Akihiro},
	title = {Marginalized Kernels Between Labeled Graphs},
	booktitle = {Proceedings of the Twentieth International Conference on International Conference on Machine Learning},
	series = {ICML'03},
	year = {2003},
	isbn = {1-57735-189-4},
	location = {Washington, DC, USA},
	pages = {321--328},
	numpages = {8},
	url = {http://dl.acm.org/citation.cfm?id=3041838.3041879},
	acmid = {3041879},
	publisher = {AAAI Press},
} 

@techreport{haussler99convolution,
	added-at = {2011-02-11T11:29:10.000+0100},
	address = {Santa Cruz, CA, USA},
	author = {Haussler, David},
	biburl = {https://www.bibsonomy.org/bibtex/266f4c8b9c3769f1718cade8f022096a4/utahell},
	description = {Convolution Kernels on Discrete Structures - Haussler (ResearchIndex)},
	institution = {University of California at Santa Cruz},
	interhash = {d730a29e3ae9a3ef0de3ff6199a4fd98},
	intrahash = {66f4c8b9c3769f1718cade8f022096a4},
	keywords = {kernel structured-data},
	number = {UCS-CRL-99-10},
	timestamp = {2011-10-28T16:52:50.000+0200},
	title = {Convolution Kernels on Discrete Structures},
	type = {Technical Report},
	url = {http://citeseer.ist.psu.edu/haussler99convolution.html},
	year = 1999
}

@book{imrich_prodgraphs,
	title = {Product graphs},
	author = {Imrich, Wilfried and Klavžar, Sandi},
	issn = {0-471-37039-8},
	year = {2000},
	date = {2000-01-01},
	pages = {xvi+358},
	publisher = {Wiley-Interscience, New York},
	series = {Wiley-Interscience Series in Discrete Mathematics and Optimization},
	note = {Structure and recognition, With a foreword by Peter Winkler},
	keywords = {},
	pubstate = {published},
	tppubtype = {book}
}

@Inbook{VanLoan1993,
	author="Van Loan, C. F.
	and Pitsianis, N.",
	editor="Moonen, Marc S.
	and Golub, Gene H.
	and De Moor, Bart L. R.",
	title="Approximation with Kronecker Products",
	bookTitle="Linear Algebra for Large Scale and Real-Time Applications",
	year="1993",
	publisher="Springer Netherlands",
	address="Dordrecht",
	pages="293--314",
	abstract="Let A be an m-by-n matrix with m = m1m2 and n = n1n2. We consider the problem of finding {\%} MathType!MTEF!2!1!+-{\%} feaaguart1ev2aaatCvAUfeBSjuyZL2yd9gzLbvyNv2CaerbuLwBLn{\%} hiov2DGi1BTfMBaeXatLxBI9gBaerbd9wDYLwzYbItLDharqqtubsr{\%} 4rNCHbGeaGqiVu0Je9sqqrpepC0xbbL8F4rqqrFfpeea0xe9Lq-Jc9{\%} vqaqpepm0xbba9pwe9Q8fs0-yqaqpepae9pg0FirpepeKkFr0xfr-x{\%} fr-xb9adbaqaaeGaciGaaiaabeqaamaabaabaaGcbaGaamOqaiabgI{\%} Giolabl2riHoaaCaaaleqabaGaamyBamaaBaaameaacaaIXaaabeaa{\%} liabgEna0kaad6gadaWgaaadbaGaaGymaaqabaaaaOGaaGjbVlaadg{\%} gacaWGUbGaamizaiaaysW7caWGdbGaaGjbVlabgIGiolaaysW7cqWI{\%} DesOdaahaaWcbeqaaiaad2gadaWgaaadbaGaaGOmaaqabaWccqGHxd{\%} aTcaWGUbWaaSbaaWqaaiaaikdaaeqaaaaakiaaysW7caWGZbGaam4B{\%} aiaaysW7caWG0bGaamiAaiaadggacaWG0bGaaGjbVlaacYhacaGG8b{\%} GaaGjbVlaadgeacqGHsislcaWGcbGaey4LIqSaam4qaiaaysW7caGG{\%} 8bGaaiiFaiaaysW7daWgaaWcbaGaamOraaqabaaaaa!6BBE!{\$}{\$}B {\backslash}in {\{}{\backslash}mathbb{\{}R{\}}^{\{}{\{}m{\_}1{\}} {\backslash}times {\{}n{\_}1{\}}{\}}{\}}{\backslash};and{\backslash};C{\backslash}; {\backslash}in {\backslash};{\{}{\backslash}mathbb{\{}R{\}}^{\{}{\{}m{\_}2{\}} {\backslash}times {\{}n{\_}2{\}}{\}}{\}}{\backslash};so{\backslash};that{\backslash};||{\backslash};A - B {\backslash}otimes C{\backslash};||{\{}{\backslash};{\_}F{\}}{\$}{\$}is minimized. This problem can be solved by computing the largest singular value and associated singular vectors of a permuted version of A. If A is symmetric, definite, non-negative, or banded, then the minimizing B and C are similarly structured. The idea of using Kronecker product preconditioned is briefly discussed.",
	isbn="978-94-015-8196-7",
	doi="10.1007/978-94-015-8196-7_17",
	url="https://doi.org/10.1007/978-94-015-8196-7_17"
}

@article{lathauwer2004,
	author = {De Lathauwer, L. and De Moor, B. and Vandewalle, J.},
	title = {Computation of the Canonical Decomposition by Means of a Simultaneous Generalized Schur Decomposition},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	volume = {26},
	number = {2},
	pages = {295-327},
	year = {2004},
	doi = {10.1137/S089547980139786X},
	
	URL = { 
	https://doi.org/10.1137/S089547980139786X
	
	},
	eprint = { 
	https://doi.org/10.1137/S089547980139786X
	
	}
	
}

